{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "notebook_header",
   "metadata": {},
   "source": [
    "# Step 08: Control Totals\n",
    "\n",
    "This notebook executes control totals SQL scripts and compares Contract Import File (3b) vs RMS EDM (3d) results.\n",
    "\n",
    "**Tasks:**\n",
    "- Load configuration metadata (date_value, cycle_type)\n",
    "- Validate Workspace EDM\n",
    "- Execute 3b_Control_Totals_Contract_Import_File_Tables.sql\n",
    "- Execute 3d_RMS_EDM_Control_Totals.sql\n",
    "- Compare 3b vs 3d results and display differences\n",
    "- Execute 3e_GeocodingSummary.sql and validate GeoHaz thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_header",
   "metadata": {},
   "source": [
    "## 1) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from helpers.notebook_setup import initialize_notebook_context\n",
    "from helpers import ux\n",
    "from helpers.database import execute_query\n",
    "from helpers.sqlserver import execute_query_from_file, sql_file_exists\n",
    "from helpers.constants import WORKSPACE_PATH\n",
    "from helpers.configuration import read_configuration\n",
    "from helpers.irp_integration import IRPClient\n",
    "from helpers.control_totals import (\n",
    "    compare_3b_vs_3d_pivot,\n",
    "    validate_geohaz_thresholds,\n",
    "    get_import_file_mapping_from_config\n",
    ")\n",
    "from helpers.excel_export import (\n",
    "    save_control_totals_3b_vs_3d_to_excel,\n",
    "    save_geohaz_validation_to_excel\n",
    ")\n",
    "from helpers.csv_export import save_dataframes_to_csv\n",
    "\n",
    "# Flag to track execution state\n",
    "execution_failed = False\n",
    "error_message = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "context_init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize notebook context and step tracking\n",
    "context, step = initialize_notebook_context('Step_08_Control_Totals.ipynb')\n",
    "\n",
    "# Display context\n",
    "ux.header(\"Control Totals Execution\")\n",
    "ux.info(f\"Cycle: {context.cycle_name}\")\n",
    "ux.info(f\"Stage: {context.stage_name}\")\n",
    "ux.info(f\"Step: {context.step_name}\")\n",
    "ux.success(f\"✓ Step tracking initialized for '{context.step_name}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "## 2) Load Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration to get date_value and cycle_type\n",
    "ux.subheader(\"Load Configuration\")\n",
    "\n",
    "# Query for active configuration\n",
    "query = \"\"\"\n",
    "    SELECT c.id, c.configuration_data\n",
    "    FROM irp_configuration c\n",
    "    INNER JOIN irp_cycle cy ON c.cycle_id = cy.id\n",
    "    WHERE cy.cycle_name = %s\n",
    "      AND c.status IN ('VALID', 'ACTIVE')\n",
    "    ORDER BY c.created_ts DESC\n",
    "    LIMIT 1\n",
    "\"\"\"\n",
    "\n",
    "result = execute_query(query, (context.cycle_name,))\n",
    "\n",
    "if result.empty:\n",
    "    execution_failed = True\n",
    "    error_message = \"No valid configuration found for this cycle\"\n",
    "    ux.error(f\"✗ {error_message}\")\n",
    "else:\n",
    "    config_id = int(result.iloc[0]['id'])\n",
    "    config_data = result.iloc[0]['configuration_data']\n",
    "    metadata = config_data.get('Metadata', {})\n",
    "    \n",
    "    date_value = metadata.get('Current Date Value', '')\n",
    "    cycle_type = metadata.get('Cycle Type', '')\n",
    "    \n",
    "    if not date_value or not cycle_type:\n",
    "        execution_failed = True\n",
    "        error_message = f\"Missing required metadata: date_value={date_value}, cycle_type={cycle_type}\"\n",
    "        ux.error(f\"✗ {error_message}\")\n",
    "    else:\n",
    "        ux.success(f\"✓ Configuration loaded\")\n",
    "        ux.info(f\"  Configuration ID: {config_id}\")\n",
    "        ux.info(f\"  Date Value: {date_value}\")\n",
    "        ux.info(f\"  Cycle Type: {cycle_type}\")\n",
    "        step.log(f\"Configuration loaded: date_value={date_value}, cycle_type={cycle_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edm_header",
   "metadata": {},
   "source": [
    "## 3) Validate Workspace EDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate_edm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Workspace EDM\n",
    "workspace_edm_full_name = None\n",
    "\n",
    "if execution_failed:\n",
    "    ux.warning(\"⏭ Skipping EDM validation due to configuration error\")\n",
    "else:\n",
    "    ux.subheader(\"Validate Workspace EDM\")\n",
    "    \n",
    "    try:\n",
    "        irp_client = IRPClient()\n",
    "        workspace_edm = \"WORKSPACE_EDM\"\n",
    "        workspace_edms = irp_client.edm.search_edms(filter=f\"exposureName=\\\"{workspace_edm}\\\"\")\n",
    "        \n",
    "        if len(workspace_edms) == 0:\n",
    "            moody_job_id = irp_client.edm.submit_create_edm_job(edm_name=workspace_edm)\n",
    "            irp_client.job.poll_risk_data_job_to_completion(moody_job_id)\n",
    "            workspace_edms = irp_client.edm.search_edms(filter=f\"exposureName=\\\"{workspace_edm}\\\"\")\n",
    "        \n",
    "        workspace_edm_obj = workspace_edms[0]\n",
    "        workspace_edm_full_name = workspace_edm_obj['databaseName']\n",
    "        \n",
    "        ux.success(f\"✓ Workspace EDM validated\")\n",
    "        ux.info(f\"  EDM Full Name: {workspace_edm_full_name}\")\n",
    "        step.log(f\"Workspace EDM: {workspace_edm_full_name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        execution_failed = True\n",
    "        error_message = f\"Failed to validate Workspace EDM: {str(e)}\"\n",
    "        ux.error(f\"✗ {error_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import_file_header",
   "metadata": {},
   "source": [
    "## 4) Execute Import File Control Totals (3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute_3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute 3b_Control_Totals_Contract_Import_File_Tables.sql\n",
    "import_file_results = []\n",
    "csv_3b_path = None\n",
    "\n",
    "if execution_failed:\n",
    "    ux.warning(\"⏭ Skipping 3b execution due to previous error\")\n",
    "else:\n",
    "    ux.subheader(\"Execute Import File Control Totals (3b)\")\n",
    "    \n",
    "    sql_file_3b = WORKSPACE_PATH / 'sql' / 'control_totals' / '3b_Control_Totals_Contract_Import_File_Tables.sql'\n",
    "    \n",
    "    if not sql_file_exists(sql_file_3b):\n",
    "        ux.warning(f\"⚠ SQL file not found: {sql_file_3b}\")\n",
    "        ux.info(\"Skipping 3b execution\")\n",
    "    else:\n",
    "        ux.info(f\"Executing: {sql_file_3b.name}\")\n",
    "        ux.info(f\"Parameters: DATE_VALUE={date_value}, CYCLE_TYPE={cycle_type}\")\n",
    "        ux.info(\"\")\n",
    "        \n",
    "        try:\n",
    "            # Execute SQL script - returns list of DataFrames (one per SELECT)\n",
    "            import_file_results = execute_query_from_file(\n",
    "                sql_file_3b,\n",
    "                params={'DATE_VALUE': date_value, 'CYCLE_TYPE': cycle_type},\n",
    "                connection='ASSURANT',\n",
    "                database='DW_EXP_MGMT_USER'\n",
    "            )\n",
    "            \n",
    "            ux.success(f\"✓ Executed 3b script: {len(import_file_results)} result set(s)\")\n",
    "            step.log(f\"Executed 3b script: {len(import_file_results)} result sets\")\n",
    "            \n",
    "            # Save combined results to CSV for investigation (comma-delimited)\n",
    "            if import_file_results:\n",
    "                notebook_dir = context.notebook_path.parent\n",
    "                df_3b_combined = pd.concat(import_file_results, ignore_index=True)\n",
    "                csv_3b_paths = save_dataframes_to_csv(\n",
    "                    df_3b_combined,\n",
    "                    f\"Control_Totals_3b_{date_value}\",\n",
    "                    output_dir=notebook_dir,\n",
    "                    delimiter=','\n",
    "                )\n",
    "                csv_3b_path = csv_3b_paths[0] if csv_3b_paths else None\n",
    "                if csv_3b_path:\n",
    "                    ux.info(f\"  Raw data saved to: {csv_3b_path.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            ux.error(f\"✗ Error executing 3b script: {str(e)}\")\n",
    "            step.log(f\"Error executing 3b script: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edm_control_totals_header",
   "metadata": {},
   "source": [
    "## 5) Execute RMS EDM Control Totals (3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute_3d",
   "metadata": {},
   "outputs": [],
   "source": "# Execute 3d_RMS_EDM_Control_Totals.sql\nedm_results = []\ndf_3d_normalized = None\ncsv_3d_path = None\n\nif execution_failed:\n    ux.warning(\"⏭ Skipping 3d execution due to previous error\")\nelif workspace_edm_full_name is None:\n    ux.warning(\"⚠ Cannot execute 3d: Workspace EDM not available\")\nelse:\n    ux.subheader(\"Execute RMS EDM Control Totals (3d)\")\n\n    sql_file_3d = WORKSPACE_PATH / 'sql' / 'control_totals' / '3d_RMS_EDM_Control_Totals.sql'\n\n    if not sql_file_exists(sql_file_3d):\n        ux.warning(f\"⚠ SQL file not found: {sql_file_3d}\")\n        ux.info(\"Skipping 3d execution\")\n    else:\n        ux.info(f\"Executing: {sql_file_3d.name}\")\n        ux.info(f\"Parameters: WORKSPACE_EDM={workspace_edm_full_name}, DATE_VALUE={date_value}, CYCLE_TYPE={cycle_type}\")\n        ux.info(\"\")\n\n        try:\n            # Execute SQL script - returns list of DataFrames (10 result sets)\n            edm_results = execute_query_from_file(\n                sql_file_3d,\n                params={\n                    'WORKSPACE_EDM': workspace_edm_full_name,\n                    'DATE_VALUE': date_value,\n                    'CYCLE_TYPE': cycle_type\n                },\n                connection='DATABRIDGE'\n            )\n\n            ux.success(f\"✓ Executed 3d script: {len(edm_results)} result set(s)\")\n            step.log(f\"Executed 3d script: {len(edm_results)} result sets\")\n\n            # Normalize the 10 result sets into a single DataFrame (merged by PORTNAME)\n            if edm_results and len(edm_results) >= 10:\n                from helpers.control_totals import normalize_3d_results\n                df_3d_normalized = normalize_3d_results(edm_results)\n\n                if df_3d_normalized is not None and not df_3d_normalized.empty:\n                    ux.success(f\"✓ Normalized 3d results: {len(df_3d_normalized)} unique PORTNAMEs\")\n\n                    # Save normalized results to CSV for investigation (comma-delimited)\n                    notebook_dir = context.notebook_path.parent\n                    csv_3d_paths = save_dataframes_to_csv(\n                        df_3d_normalized,\n                        f\"Control_Totals_3d_{date_value}\",\n                        output_dir=notebook_dir,\n                        delimiter=','\n                    )\n                    csv_3d_path = csv_3d_paths[0] if csv_3d_paths else None\n                    if csv_3d_path:\n                        ux.info(f\"  Normalized data saved to: {csv_3d_path.name}\")\n\n        except Exception as e:\n            ux.error(f\"✗ Error executing 3d script: {str(e)}\")\n            step.log(f\"Error executing 3d script: {str(e)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "comparison_header",
   "metadata": {},
   "source": [
    "## 6) Compare Import File (3b) vs RMS EDM (3d) Control Totals\n",
    "\n",
    "Compares control totals between 3b (Contract Import File) and 3d (RMS EDM):\n",
    "\n",
    "**Non-Flood perils** (7 attributes): PolicyCount, PolicyPremium, PolicyLimit, LocationCountDistinct, TotalReplacementValue, LocationLimit, LocationDeductible\n",
    "\n",
    "**Flood perils** (10 attributes): Adds AttachmentPoint, PolicyDeductible, PolicySublimit\n",
    "\n",
    "**A difference of 0 means the values match.** Detailed results are exported to Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compare_3b_vs_3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 3b vs 3d control totals\n",
    "comparison_results = None\n",
    "all_matched = False\n",
    "non_flood_summary = {}\n",
    "flood_summary = {}\n",
    "\n",
    "if execution_failed:\n",
    "    ux.warning(\"⏭ Skipping comparison due to previous error\")\n",
    "elif not import_file_results or not edm_results:\n",
    "    ux.warning(\"⚠ Cannot compare: Missing 3b or 3d results\")\n",
    "else:\n",
    "    ux.subheader(\"3b vs 3d Comparison Results\")\n",
    "    \n",
    "    try:\n",
    "        # Run comparison (pivot format - one row per ExposureGroup)\n",
    "        comparison_results, all_matched = compare_3b_vs_3d_pivot(\n",
    "            import_file_results,\n",
    "            edm_results\n",
    "        )\n",
    "        \n",
    "        if comparison_results is not None and not comparison_results.empty:\n",
    "            # Split into Flood and Non-Flood for separate summaries\n",
    "            is_flood = comparison_results['ExposureGroup'].str.startswith('USFL_')\n",
    "            flood_results = comparison_results[is_flood]\n",
    "            non_flood_results = comparison_results[~is_flood]\n",
    "            \n",
    "            # Non-Flood summary\n",
    "            if not non_flood_results.empty:\n",
    "                non_flood_total = len(non_flood_results)\n",
    "                non_flood_matched = (non_flood_results['Status'] == 'MATCH').sum()\n",
    "                non_flood_mismatched = non_flood_total - non_flood_matched\n",
    "                non_flood_summary = {\n",
    "                    'total': non_flood_total,\n",
    "                    'matched': int(non_flood_matched),\n",
    "                    'mismatched': int(non_flood_mismatched)\n",
    "                }\n",
    "                \n",
    "                ux.info(\"Non-Flood Perils:\")\n",
    "                if non_flood_mismatched == 0:\n",
    "                    ux.success(f\"  ✓ All {non_flood_total} exposure groups match\")\n",
    "                else:\n",
    "                    ux.warning(f\"  ⚠ {non_flood_mismatched} of {non_flood_total} exposure groups have mismatches\")\n",
    "            \n",
    "            # Flood summary\n",
    "            if not flood_results.empty:\n",
    "                flood_total = len(flood_results)\n",
    "                flood_matched = (flood_results['Status'] == 'MATCH').sum()\n",
    "                flood_mismatched = flood_total - flood_matched\n",
    "                flood_summary = {\n",
    "                    'total': flood_total,\n",
    "                    'matched': int(flood_matched),\n",
    "                    'mismatched': int(flood_mismatched)\n",
    "                }\n",
    "                \n",
    "                ux.info(\"Flood Perils (USFL_*):\")\n",
    "                if flood_mismatched == 0:\n",
    "                    ux.success(f\"  ✓ All {flood_total} exposure groups match\")\n",
    "                else:\n",
    "                    ux.warning(f\"  ⚠ {flood_mismatched} of {flood_total} exposure groups have mismatches\")\n",
    "            \n",
    "            # Log to step\n",
    "            total_groups = len(comparison_results)\n",
    "            matched_groups = (comparison_results['Status'] == 'MATCH').sum()\n",
    "            step.log(f\"3b vs 3d comparison complete: {matched_groups}/{total_groups} groups matched\")\n",
    "        else:\n",
    "            ux.warning(\"No comparison results generated\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        ux.error(f\"✗ Error comparing control totals: {str(e)}\")\n",
    "        step.log(f\"Error comparing control totals: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "export_comparison_header",
   "metadata": {},
   "source": [
    "### Export Comparison Results to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_comparison_excel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comparison results to Excel\n",
    "excel_3b_vs_3d_path = None\n",
    "\n",
    "if comparison_results is not None and not comparison_results.empty:\n",
    "    # Get the notebook directory for output\n",
    "    notebook_dir = context.notebook_path.parent\n",
    "    \n",
    "    excel_3b_vs_3d_path = save_control_totals_3b_vs_3d_to_excel(\n",
    "        comparison_results=comparison_results,\n",
    "        date_value=date_value,\n",
    "        output_dir=notebook_dir\n",
    "    )\n",
    "    \n",
    "    if excel_3b_vs_3d_path:\n",
    "        ux.success(f\"✓ Comparison results exported to: {excel_3b_vs_3d_path.name}\")\n",
    "        step.log(f\"3b vs 3d comparison exported to: {excel_3b_vs_3d_path}\")\n",
    "else:\n",
    "    ux.info(\"No comparison results to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geohaz_header",
   "metadata": {},
   "source": [
    "## 7) Execute GeoHaz Control Totals (3e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "execute_3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute GeoHaz Control Totals Query\n",
    "geocoding_results = None\n",
    "csv_3e_path = None\n",
    "\n",
    "if execution_failed:\n",
    "    ux.warning(\"⏭ Skipping 3e execution due to previous error\")\n",
    "elif workspace_edm_full_name is None:\n",
    "    ux.warning(\"⚠ Cannot execute 3e: Workspace EDM not available\")\n",
    "else:\n",
    "    ux.subheader(\"Execute GeoHaz Control Totals (3e)\")\n",
    "    \n",
    "    sql_file_3e = WORKSPACE_PATH / 'sql' / 'control_totals' / '3e_GeocodingSummary.sql'\n",
    "    \n",
    "    if not sql_file_exists(sql_file_3e):\n",
    "        ux.warning(f\"⚠ SQL file not found: {sql_file_3e}\")\n",
    "        ux.info(\"Skipping 3e execution\")\n",
    "    else:\n",
    "        ux.info(f\"Executing: {sql_file_3e.name}\")\n",
    "        ux.info(f\"Parameters: WORKSPACE_EDM={workspace_edm_full_name}, DATE_VALUE={date_value}, CYCLE_TYPE={cycle_type}\")\n",
    "        ux.info(\"\")\n",
    "        \n",
    "        try:\n",
    "            result = execute_query_from_file(\n",
    "                sql_file_3e,\n",
    "                params={\n",
    "                    'WORKSPACE_EDM': workspace_edm_full_name,\n",
    "                    'CYCLE_TYPE': cycle_type,\n",
    "                    'DATE_VALUE': date_value\n",
    "                },\n",
    "                connection='DATABRIDGE'\n",
    "            )\n",
    "            \n",
    "            # Extract geocoding results (first result set)\n",
    "            geocoding_results = result[0] if isinstance(result, list) else result\n",
    "            \n",
    "            ux.success(f\"✓ Executed 3e script: {len(geocoding_results)} geocoding records retrieved\")\n",
    "            step.log(f\"Executed 3e script: {len(geocoding_results)} geocoding records\")\n",
    "            \n",
    "            # Save results to CSV for investigation (comma-delimited)\n",
    "            if geocoding_results is not None and not geocoding_results.empty:\n",
    "                notebook_dir = context.notebook_path.parent\n",
    "                csv_3e_paths = save_dataframes_to_csv(\n",
    "                    geocoding_results,\n",
    "                    f\"Control_Totals_3e_{date_value}\",\n",
    "                    output_dir=notebook_dir,\n",
    "                    delimiter=','\n",
    "                )\n",
    "                csv_3e_path = csv_3e_paths[0] if csv_3e_paths else None\n",
    "                if csv_3e_path:\n",
    "                    ux.info(f\"  Raw data saved to: {csv_3e_path.name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            ux.error(f\"✗ Error executing 3e script: {str(e)}\")\n",
    "            step.log(f\"Error executing 3e script: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geohaz_validation_header",
   "metadata": {},
   "source": [
    "### Validate GeoHaz Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "validate_geohaz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate geocoding results against configuration thresholds\n",
    "validation_results = None\n",
    "geohaz_all_passed = False\n",
    "geohaz_summary = {}\n",
    "\n",
    "if execution_failed:\n",
    "    ux.warning(\"⏭ Skipping GeoHaz validation due to previous error\")\n",
    "elif geocoding_results is None or geocoding_results.empty:\n",
    "    ux.warning(\"⚠ No geocoding results to validate\")\n",
    "else:\n",
    "    ux.header(\"GeoHaz Threshold Validation\")\n",
    "    \n",
    "    # Get GeoHaz thresholds from configuration\n",
    "    geohaz_thresholds = config_data.get('GeoHaz Thresholds', [])\n",
    "    \n",
    "    if not geohaz_thresholds:\n",
    "        ux.warning(\"⚠ No GeoHaz thresholds found in configuration - skipping validation\")\n",
    "    else:\n",
    "        # Get portfolio to import file mapping from configuration\n",
    "        import_file_mapping = get_import_file_mapping_from_config(config_data)\n",
    "        \n",
    "        # Perform validation\n",
    "        validation_results, geohaz_all_passed = validate_geohaz_thresholds(\n",
    "            geocoding_results=geocoding_results,\n",
    "            geohaz_thresholds=geohaz_thresholds,\n",
    "            import_file_mapping=import_file_mapping\n",
    "        )\n",
    "        \n",
    "        # Handle empty validation results (e.g., no matching portfolios or all portfolios empty)\n",
    "        if validation_results is None or validation_results.empty:\n",
    "            ux.warning(\"⚠ No geocoding results matched the configured thresholds\")\n",
    "            ux.info(\"This may occur when:\")\n",
    "            ux.info(\"  - No portfolios in geocoding results match Import File names in thresholds\")\n",
    "            ux.info(\"  - All matching portfolios had zero locations\")\n",
    "            step.log(\"GeoHaz validation: No matching results to validate\")\n",
    "        else:\n",
    "            # Display summary\n",
    "            total_checks = len(validation_results)\n",
    "            passed_checks = len(validation_results[validation_results['Status'] == 'PASS'])\n",
    "            failed_checks = len(validation_results[validation_results['Status'] == 'FAIL'])\n",
    "            \n",
    "            geohaz_summary = {\n",
    "                'total_checks': int(total_checks),\n",
    "                'passed_checks': int(passed_checks),\n",
    "                'failed_checks': int(failed_checks),\n",
    "                'all_passed': bool(geohaz_all_passed)\n",
    "            }\n",
    "            \n",
    "            summary_data = [\n",
    "                [\"Total Checks\", total_checks],\n",
    "                [\"Passed\", passed_checks],\n",
    "                [\"Failed\", failed_checks]\n",
    "            ]\n",
    "            ux.table(summary_data, headers=[\"Metric\", \"Count\"])\n",
    "            \n",
    "            if geohaz_all_passed:\n",
    "                ux.success(\"✓ All geocoding thresholds met!\")\n",
    "            else:\n",
    "                ux.error(f\"✗ {failed_checks} geocoding threshold(s) not met\")\n",
    "            \n",
    "            # Display detailed results\n",
    "            ux.subheader(\"Detailed Validation Results\")\n",
    "            \n",
    "            # Show failures first\n",
    "            failures = validation_results[validation_results['Status'] == 'FAIL']\n",
    "            if not failures.empty:\n",
    "                ux.warning(f\"Threshold Violations ({len(failures)}):\")\n",
    "                print(failures.to_string(index=False))\n",
    "                print()\n",
    "            \n",
    "            # Show passes\n",
    "            passes = validation_results[validation_results['Status'] == 'PASS']\n",
    "            if not passes.empty:\n",
    "                ux.info(f\"Passed Thresholds ({len(passes)}):\")\n",
    "                print(passes.to_string(index=False))\n",
    "            \n",
    "            # Log validation status\n",
    "            if geohaz_all_passed:\n",
    "                step.log(f\"GeoHaz validation: All {total_checks} thresholds met\")\n",
    "            else:\n",
    "                step.log(f\"GeoHaz validation: {failed_checks}/{total_checks} thresholds failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geohaz_export_header",
   "metadata": {},
   "source": [
    "### Export GeoHaz Validation to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export_geohaz_excel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export validation results to Excel (one sheet per Import File)\n",
    "excel_geohaz_path = None\n",
    "\n",
    "if validation_results is not None and not validation_results.empty:\n",
    "    # Get the notebook directory for output\n",
    "    notebook_dir = context.notebook_path.parent\n",
    "    \n",
    "    excel_geohaz_path = save_geohaz_validation_to_excel(\n",
    "        validation_results=validation_results,\n",
    "        date_value=date_value,\n",
    "        cycle_type=cycle_type,\n",
    "        output_dir=notebook_dir\n",
    "    )\n",
    "    \n",
    "    if excel_geohaz_path:\n",
    "        ux.success(f\"✓ GeoHaz validation exported to: {excel_geohaz_path.name}\")\n",
    "        step.log(f\"GeoHaz validation exported to: {excel_geohaz_path}\")\n",
    "else:\n",
    "    ux.info(\"No GeoHaz validation results to export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete_header",
   "metadata": {},
   "source": [
    "## 8) Complete Step Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete_step",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete step execution\n",
    "ux.header(\"Step Completion\")\n",
    "\n",
    "if execution_failed:\n",
    "    # Handle configuration/execution failure\n",
    "    from helpers.step import update_step_run\n",
    "    from helpers.constants import StepStatus\n",
    "    \n",
    "    update_step_run(step.run_id, StepStatus.FAILED, error_message=error_message)\n",
    "    \n",
    "    ux.error(\"\\n\" + \"=\"*60)\n",
    "    ux.error(\"CONTROL TOTALS EXECUTION FAILED\")\n",
    "    ux.error(\"=\"*60)\n",
    "    ux.error(f\"\\nError: {error_message}\")\n",
    "    ux.info(\"\\nPlease fix the error and retry.\")\n",
    "\n",
    "else:\n",
    "    # Build comparison summary for output\n",
    "    comparison_summary = {}\n",
    "    if comparison_results is not None and not comparison_results.empty:\n",
    "        total_groups = len(comparison_results)\n",
    "        matched_groups = int((comparison_results['Status'] == 'MATCH').sum())\n",
    "        comparison_summary = {\n",
    "            'total_exposure_groups': total_groups,\n",
    "            'matched_groups': matched_groups,\n",
    "            'mismatched_groups': total_groups - matched_groups,\n",
    "            'all_matched': bool(all_matched),\n",
    "            'non_flood': non_flood_summary,\n",
    "            'flood': flood_summary\n",
    "        }\n",
    "    \n",
    "    # Complete the step successfully\n",
    "    output_data = {\n",
    "        'date_value': date_value,\n",
    "        'cycle_type': cycle_type,\n",
    "        'import_file_result_count': len(import_file_results),\n",
    "        'edm_result_count': len(edm_results),\n",
    "        '3b_vs_3d_comparison': comparison_summary,\n",
    "        'geohaz_validation': geohaz_summary\n",
    "    }\n",
    "    step.complete(output_data)\n",
    "\n",
    "    ux.success(\"\\n\" + \"=\"*60)\n",
    "    ux.success(\"✓ CONTROL TOTALS VALIDATION COMPLETED\")\n",
    "    ux.success(\"=\"*60)\n",
    "    \n",
    "    # Display 3b vs 3d summary\n",
    "    ux.info(f\"\\nImport File (3b): {len(import_file_results)} result set(s)\")\n",
    "    ux.info(f\"RMS EDM (3d): {len(edm_results)} result set(s)\")\n",
    "    \n",
    "    if comparison_summary:\n",
    "        ux.info(f\"\\n3b vs 3d Comparison:\")\n",
    "        \n",
    "        # Non-Flood summary\n",
    "        if non_flood_summary:\n",
    "            if non_flood_summary['mismatched'] == 0:\n",
    "                ux.success(f\"  Non-Flood: ✓ All {non_flood_summary['total']} groups match\")\n",
    "            else:\n",
    "                ux.warning(f\"  Non-Flood: ⚠ {non_flood_summary['mismatched']}/{non_flood_summary['total']} groups have mismatches\")\n",
    "        \n",
    "        # Flood summary\n",
    "        if flood_summary:\n",
    "            if flood_summary['mismatched'] == 0:\n",
    "                ux.success(f\"  Flood: ✓ All {flood_summary['total']} groups match\")\n",
    "            else:\n",
    "                ux.warning(f\"  Flood: ⚠ {flood_summary['mismatched']}/{flood_summary['total']} groups have mismatches\")\n",
    "    \n",
    "    # Display GeoHaz summary\n",
    "    if geohaz_summary:\n",
    "        ux.info(f\"\\nGeoHaz Validation: {geohaz_summary['passed_checks']}/{geohaz_summary['total_checks']} checks passed\")\n",
    "        if geohaz_summary['all_passed']:\n",
    "            ux.success(\"  ✓ All geocoding thresholds met\")\n",
    "        else:\n",
    "            ux.warning(f\"  ⚠ {geohaz_summary['failed_checks']} threshold(s) not met\")\n",
    "    elif validation_results is not None and validation_results.empty:\n",
    "        ux.info(\"\\nGeoHaz validation: No matching results to validate\")\n",
    "    else:\n",
    "        ux.info(\"\\nGeoHaz validation: Skipped (no thresholds configured or no results)\")\n",
    "    \n",
    "    # Display exported files\n",
    "    ux.info(\"\\nExported Files:\")\n",
    "    if csv_3b_path:\n",
    "        ux.info(f\"  - {csv_3b_path.name}\")\n",
    "    if csv_3d_path:\n",
    "        ux.info(f\"  - {csv_3d_path.name}\")\n",
    "    if csv_3e_path:\n",
    "        ux.info(f\"  - {csv_3e_path.name}\")\n",
    "    if excel_3b_vs_3d_path:\n",
    "        ux.info(f\"  - {excel_3b_vs_3d_path.name}\")\n",
    "    if excel_geohaz_path:\n",
    "        ux.info(f\"  - {excel_geohaz_path.name}\")\n",
    "    \n",
    "    ux.info(\"\\nNext: Proceed to Stage 04 (Analysis Execution)\")\n",
    "\n",
    "    # Send Teams notification for milestone completion\n",
    "    try:\n",
    "        from helpers.teams_notification import TeamsNotificationClient, build_notification_actions\n",
    "        from helpers.database import get_current_schema\n",
    "\n",
    "        teams = TeamsNotificationClient()\n",
    "        schema = get_current_schema()\n",
    "        actions = build_notification_actions(\n",
    "            notebook_path=str(context.notebook_path),\n",
    "            cycle_name=context.cycle_name,\n",
    "            schema=schema\n",
    "        )\n",
    "        \n",
    "        # Build comparison status message\n",
    "        comparison_msg = \"\"\n",
    "        if comparison_summary:\n",
    "            if non_flood_summary:\n",
    "                if non_flood_summary['mismatched'] == 0:\n",
    "                    comparison_msg += f\"- Non-Flood: ✓ All {non_flood_summary['total']} groups match\\n\"\n",
    "                else:\n",
    "                    comparison_msg += f\"- Non-Flood: ⚠ {non_flood_summary['mismatched']}/{non_flood_summary['total']} groups have mismatches\\n\"\n",
    "            if flood_summary:\n",
    "                if flood_summary['mismatched'] == 0:\n",
    "                    comparison_msg += f\"- Flood: ✓ All {flood_summary['total']} groups match\\n\"\n",
    "                else:\n",
    "                    comparison_msg += f\"- Flood: ⚠ {flood_summary['mismatched']}/{flood_summary['total']} groups have mismatches\\n\"\n",
    "        \n",
    "        # Build GeoHaz status message\n",
    "        geohaz_msg = \"\"\n",
    "        if geohaz_summary:\n",
    "            if geohaz_summary['all_passed']:\n",
    "                geohaz_msg = f\"- GeoHaz: ✓ All {geohaz_summary['total_checks']} thresholds met\\n\"\n",
    "            else:\n",
    "                geohaz_msg = f\"- GeoHaz: ⚠ {geohaz_summary['failed_checks']}/{geohaz_summary['total_checks']} thresholds not met\\n\"\n",
    "\n",
    "        teams.send_success(\n",
    "            title=f\"[{context.cycle_name}] Stage 03 Control Totals Complete\",\n",
    "            message=f\"**Cycle:** {context.cycle_name}\\n\"\n",
    "                    f\"**Stage:** {context.stage_name}\\n\"\n",
    "                    f\"**Step:** {context.step_name}\\n\\n\"\n",
    "                    f\"**Results:**\\n\"\n",
    "                    f\"- Import File (3b): {len(import_file_results)} result set(s)\\n\"\n",
    "                    f\"- RMS EDM (3d): {len(edm_results)} result set(s)\\n\\n\"\n",
    "                    f\"**3b vs 3d Comparison:**\\n\"\n",
    "                    f\"{comparison_msg}\\n\"\n",
    "                    f\"**GeoHaz Validation:**\\n\"\n",
    "                    f\"{geohaz_msg}\\n\"\n",
    "                    f\"Ready to proceed to Stage 04 Analysis Execution.\",\n",
    "            actions=actions\n",
    "        )\n",
    "        ux.info(\"\\nTeams notification sent.\")\n",
    "    except Exception as e:\n",
    "        ux.warning(f\"\\nCould not send Teams notification: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}