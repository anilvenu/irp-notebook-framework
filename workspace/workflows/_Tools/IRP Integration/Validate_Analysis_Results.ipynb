{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Validate Analysis Results\n",
    "\n",
    "This notebook compares analysis outputs between a **production run** (baseline) and a **test run** to validate that the test produced expected results.\n",
    "\n",
    "**Endpoints compared:**\n",
    "- Statistics (`/stats`)\n",
    "- EP Metrics (`/ep`)\n",
    "- Event Loss Table (`/elt`)\n",
    "- Period Loss Table (`/plt`) - HD analyses only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import math\n",
    "from typing import Dict, List, Any, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from helpers.irp_integration import IRPClient\n",
    "\n",
    "irp_client = IRPClient()\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config-header",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Enter the **appAnalysisId** values for the production and test analyses to compare.\n",
    "\n",
    "The `appAnalysisId` is the ID shown in the Moody's RiskModeler UI (e.g., 35810)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis IDs to compare\n",
    "PRODUCTION_APP_ANALYSIS_ID = 35839  # Replace with your production analysis ID\n",
    "TEST_APP_ANALYSIS_ID = 35662        # Replace with your test analysis ID\n",
    "\n",
    "# Perspective code: 'GR' (Gross), 'GU' (Ground-Up), 'RL' (Reinsurance Layer)\n",
    "PERSPECTIVE_CODE = 'GU'\n",
    "\n",
    "# Include PLT comparison? (only for HD analyses)\n",
    "INCLUDE_PLT = False\n",
    "\n",
    "# Comparison settings\n",
    "RELATIVE_TOLERANCE = 1e-9  # For floating-point comparison\n",
    "MAX_DIFFERENCES_TO_SHOW = 50  # Limit output for large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpers-header",
   "metadata": {},
   "source": [
    "## 3. Comparison Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "helpers",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded.\n",
      "Ignoring metadata fields: ['analysisId', 'appAnalysisId', 'createdAt', 'createdBy', 'exposureResourceId', 'exposureResourceType', 'jobId', 'modifiedAt', 'modifiedBy', 'perspectiveCode', 'uri']\n"
     ]
    }
   ],
   "source": [
    "# Fields to IGNORE when comparing (metadata, not analysis results)\n",
    "IGNORED_FIELDS = {\n",
    "    'analysisId',\n",
    "    'jobId', \n",
    "    'uri',\n",
    "    'exposureResourceId',\n",
    "    'exposureResourceType',\n",
    "    'perspectiveCode',\n",
    "    'appAnalysisId',\n",
    "    'createdAt',\n",
    "    'modifiedAt',\n",
    "    'createdBy',\n",
    "    'modifiedBy',\n",
    "}\n",
    "\n",
    "# Meaningful fields to compare per endpoint (allowlist approach)\n",
    "# If empty, compare all fields except IGNORED_FIELDS\n",
    "ELT_FIELDS = {\n",
    "    'eventId',\n",
    "    'positionValue',\n",
    "    'stdDevI', \n",
    "    'stdDevC',\n",
    "    'exposureValue',\n",
    "    'eventRate',\n",
    "}\n",
    "\n",
    "EP_FIELDS = {\n",
    "    'epType',           # OEP, AEP, CEP, TCE-OEP, TCE-AEP\n",
    "    'returnPeriod',\n",
    "    'loss',\n",
    "    'lossValue',\n",
    "    'probability',\n",
    "}\n",
    "\n",
    "STATS_FIELDS = {\n",
    "    'metricCode',\n",
    "    'metricName', \n",
    "    'value',\n",
    "    'stdDev',\n",
    "    'mean',\n",
    "    'aep',\n",
    "    'oep',\n",
    "    'aal',              # Average Annual Loss\n",
    "    'stdDevAal',\n",
    "    'cv',               # Coefficient of Variation\n",
    "}\n",
    "\n",
    "PLT_FIELDS = {\n",
    "    'eventId',\n",
    "    'eventDate',\n",
    "    'lossDate',\n",
    "    'loss',\n",
    "    'lossValue',\n",
    "    'positionValue',\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ComparisonResult:\n",
    "    \"\"\"Result of comparing two datasets.\"\"\"\n",
    "    endpoint: str\n",
    "    passed: bool\n",
    "    total_records_prod: int\n",
    "    total_records_test: int\n",
    "    differences: List[Dict[str, Any]]\n",
    "    missing_in_test: List[Any]\n",
    "    extra_in_test: List[Any]\n",
    "    error: Optional[str] = None\n",
    "\n",
    "\n",
    "def values_match(a: Any, b: Any, rel_tol: float = 1e-9) -> bool:\n",
    "    \"\"\"Compare two values with tolerance for floats.\"\"\"\n",
    "    if a is None and b is None:\n",
    "        return True\n",
    "    if a is None or b is None:\n",
    "        return False\n",
    "    if isinstance(a, (int, float)) and isinstance(b, (int, float)):\n",
    "        if a == 0 and b == 0:\n",
    "            return True\n",
    "        return math.isclose(a, b, rel_tol=rel_tol)\n",
    "    return a == b\n",
    "\n",
    "\n",
    "def compare_records(\n",
    "    prod_record: Dict[str, Any],\n",
    "    test_record: Dict[str, Any],\n",
    "    key_field: str,\n",
    "    fields_to_compare: set = None,\n",
    "    rel_tol: float = 1e-9\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Compare two records and return list of field differences.\n",
    "    \n",
    "    Args:\n",
    "        prod_record: Production record\n",
    "        test_record: Test record  \n",
    "        key_field: Field used as key (will be skipped in comparison)\n",
    "        fields_to_compare: If provided, only compare these fields. \n",
    "                          If None, compare all fields except IGNORED_FIELDS.\n",
    "        rel_tol: Relative tolerance for float comparison\n",
    "    \"\"\"\n",
    "    differences = []\n",
    "    \n",
    "    # Determine which fields to compare\n",
    "    if fields_to_compare:\n",
    "        # Use allowlist - only compare specified fields (excluding key)\n",
    "        all_keys = fields_to_compare - {key_field}\n",
    "    else:\n",
    "        # Compare all fields except ignored ones and key\n",
    "        all_keys = (set(prod_record.keys()) | set(test_record.keys())) - IGNORED_FIELDS - {key_field}\n",
    "    \n",
    "    for key in all_keys:\n",
    "        prod_val = prod_record.get(key)\n",
    "        test_val = test_record.get(key)\n",
    "        \n",
    "        if not values_match(prod_val, test_val, rel_tol):\n",
    "            differences.append({\n",
    "                'field': key,\n",
    "                'prod_value': prod_val,\n",
    "                'test_value': test_val\n",
    "            })\n",
    "    \n",
    "    return differences\n",
    "\n",
    "\n",
    "def compare_datasets(\n",
    "    prod_data: List[Dict[str, Any]],\n",
    "    test_data: List[Dict[str, Any]],\n",
    "    key_field: str,\n",
    "    endpoint_name: str,\n",
    "    fields_to_compare: set = None,\n",
    "    rel_tol: float = 1e-9\n",
    ") -> ComparisonResult:\n",
    "    \"\"\"Compare two datasets by matching on key_field.\"\"\"\n",
    "    # Build lookup dictionaries\n",
    "    prod_by_key = {r.get(key_field): r for r in prod_data}\n",
    "    test_by_key = {r.get(key_field): r for r in test_data}\n",
    "    \n",
    "    prod_keys = set(prod_by_key.keys())\n",
    "    test_keys = set(test_by_key.keys())\n",
    "    \n",
    "    # Find missing/extra records\n",
    "    missing_in_test = list(prod_keys - test_keys)\n",
    "    extra_in_test = list(test_keys - prod_keys)\n",
    "    common_keys = prod_keys & test_keys\n",
    "    \n",
    "    # Compare common records\n",
    "    all_differences = []\n",
    "    for key in common_keys:\n",
    "        diffs = compare_records(\n",
    "            prod_by_key[key], \n",
    "            test_by_key[key], \n",
    "            key_field, \n",
    "            fields_to_compare,\n",
    "            rel_tol\n",
    "        )\n",
    "        if diffs:\n",
    "            all_differences.append({\n",
    "                'key': key,\n",
    "                'differences': diffs\n",
    "            })\n",
    "    \n",
    "    passed = (len(missing_in_test) == 0 and \n",
    "              len(extra_in_test) == 0 and \n",
    "              len(all_differences) == 0)\n",
    "    \n",
    "    return ComparisonResult(\n",
    "        endpoint=endpoint_name,\n",
    "        passed=passed,\n",
    "        total_records_prod=len(prod_data),\n",
    "        total_records_test=len(test_data),\n",
    "        differences=all_differences,\n",
    "        missing_in_test=missing_in_test,\n",
    "        extra_in_test=extra_in_test\n",
    "    )\n",
    "\n",
    "\n",
    "def compare_by_index(\n",
    "    prod_data: List[Dict[str, Any]],\n",
    "    test_data: List[Dict[str, Any]],\n",
    "    endpoint_name: str,\n",
    "    fields_to_compare: set = None,\n",
    "    rel_tol: float = 1e-9\n",
    ") -> ComparisonResult:\n",
    "    \"\"\"Compare data by index position (for stats/EP without unique keys).\"\"\"\n",
    "    if len(prod_data) != len(test_data):\n",
    "        return ComparisonResult(\n",
    "            endpoint=endpoint_name,\n",
    "            passed=False,\n",
    "            total_records_prod=len(prod_data),\n",
    "            total_records_test=len(test_data),\n",
    "            differences=[],\n",
    "            missing_in_test=[],\n",
    "            extra_in_test=[],\n",
    "            error=f\"Record count mismatch: prod={len(prod_data)}, test={len(test_data)}\"\n",
    "        )\n",
    "    \n",
    "    all_differences = []\n",
    "    for i, (prod_rec, test_rec) in enumerate(zip(prod_data, test_data)):\n",
    "        diffs = compare_records(\n",
    "            prod_rec, \n",
    "            test_rec, \n",
    "            key_field='_index_', \n",
    "            fields_to_compare=fields_to_compare,\n",
    "            rel_tol=rel_tol\n",
    "        )\n",
    "        if diffs:\n",
    "            all_differences.append({\n",
    "                'key': f'record_{i}',\n",
    "                'differences': diffs\n",
    "            })\n",
    "    \n",
    "    return ComparisonResult(\n",
    "        endpoint=endpoint_name,\n",
    "        passed=len(all_differences) == 0,\n",
    "        total_records_prod=len(prod_data),\n",
    "        total_records_test=len(test_data),\n",
    "        differences=all_differences,\n",
    "        missing_in_test=[],\n",
    "        extra_in_test=[]\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded.\")\n",
    "print(f\"Ignoring metadata fields: {sorted(IGNORED_FIELDS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fetch-header",
   "metadata": {},
   "source": [
    "## 4. Fetch Analysis Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fetch-metadata",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching analysis metadata...\n",
      "\n",
      "Production Analysis:\n",
      "  appAnalysisId: 35839\n",
      "  analysisId: 3428895\n",
      "  analysisName: RM_EDM_202511_Quarterly_USFL: PORTFOLIO: USFL_Other_Other\n",
      "  exposureResourceId: 8\n",
      "\n",
      "Test Analysis:\n",
      "  appAnalysisId: 35662\n",
      "  analysisId: 3405266\n",
      "  analysisName: USFL_Other_Other_LT\n",
      "  exposureResourceId: 8\n",
      "\n",
      "Perspective: GU\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching analysis metadata...\")\n",
    "print()\n",
    "\n",
    "# Fetch production analysis\n",
    "prod_analysis = irp_client.analysis.get_analysis_by_app_analysis_id(PRODUCTION_APP_ANALYSIS_ID)\n",
    "prod_analysis_id = prod_analysis['analysisId']\n",
    "prod_exposure_resource_id = prod_analysis['exposureResourceId']\n",
    "print(f\"Production Analysis:\")\n",
    "print(f\"  appAnalysisId: {PRODUCTION_APP_ANALYSIS_ID}\")\n",
    "print(f\"  analysisId: {prod_analysis_id}\")\n",
    "print(f\"  analysisName: {prod_analysis['analysisName']}\")\n",
    "print(f\"  exposureResourceId: {prod_exposure_resource_id}\")\n",
    "print()\n",
    "\n",
    "# Fetch test analysis\n",
    "test_analysis = irp_client.analysis.get_analysis_by_app_analysis_id(TEST_APP_ANALYSIS_ID)\n",
    "test_analysis_id = test_analysis['analysisId']\n",
    "test_exposure_resource_id = test_analysis['exposureResourceId']\n",
    "print(f\"Test Analysis:\")\n",
    "print(f\"  appAnalysisId: {TEST_APP_ANALYSIS_ID}\")\n",
    "print(f\"  analysisId: {test_analysis_id}\")\n",
    "print(f\"  analysisName: {test_analysis['analysisName']}\")\n",
    "print(f\"  exposureResourceId: {test_exposure_resource_id}\")\n",
    "print()\n",
    "print(f\"Perspective: {PERSPECTIVE_CODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compare-header",
   "metadata": {},
   "source": [
    "## 5. Fetch and Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "compare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Statistics...\n",
      "  Production: 1 records\n",
      "  Test: 1 records\n",
      "\n",
      "Fetching EP Metrics...\n",
      "  Production: 4 records\n",
      "  Test: 4 records\n",
      "\n",
      "Fetching ELT...\n",
      "  Production: 100 events\n",
      "  Test: 100 events\n",
      "\n",
      "Comparison complete.\n"
     ]
    }
   ],
   "source": [
    "results: List[ComparisonResult] = []\n",
    "\n",
    "# --- Statistics ---\n",
    "print(\"Fetching Statistics...\")\n",
    "try:\n",
    "    prod_stats = irp_client.analysis.get_stats(prod_analysis_id, PERSPECTIVE_CODE, prod_exposure_resource_id)\n",
    "    test_stats = irp_client.analysis.get_stats(test_analysis_id, PERSPECTIVE_CODE, test_exposure_resource_id)\n",
    "    stats_result = compare_by_index(prod_stats, test_stats, 'Statistics', STATS_FIELDS, RELATIVE_TOLERANCE)\n",
    "    results.append(stats_result)\n",
    "    print(f\"  Production: {len(prod_stats)} records\")\n",
    "    print(f\"  Test: {len(test_stats)} records\")\n",
    "except Exception as e:\n",
    "    results.append(ComparisonResult(\n",
    "        endpoint='Statistics', passed=False, total_records_prod=0, total_records_test=0,\n",
    "        differences=[], missing_in_test=[], extra_in_test=[], error=str(e)\n",
    "    ))\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "# --- EP Metrics ---\n",
    "print(\"\\nFetching EP Metrics...\")\n",
    "try:\n",
    "    prod_ep = irp_client.analysis.get_ep(prod_analysis_id, PERSPECTIVE_CODE, prod_exposure_resource_id)\n",
    "    test_ep = irp_client.analysis.get_ep(test_analysis_id, PERSPECTIVE_CODE, test_exposure_resource_id)\n",
    "    ep_result = compare_by_index(prod_ep, test_ep, 'EP Metrics', EP_FIELDS, RELATIVE_TOLERANCE)\n",
    "    results.append(ep_result)\n",
    "    print(f\"  Production: {len(prod_ep)} records\")\n",
    "    print(f\"  Test: {len(test_ep)} records\")\n",
    "except Exception as e:\n",
    "    results.append(ComparisonResult(\n",
    "        endpoint='EP Metrics', passed=False, total_records_prod=0, total_records_test=0,\n",
    "        differences=[], missing_in_test=[], extra_in_test=[], error=str(e)\n",
    "    ))\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "# --- ELT ---\n",
    "print(\"\\nFetching ELT...\")\n",
    "try:\n",
    "    prod_elt = irp_client.analysis.get_elt(prod_analysis_id, PERSPECTIVE_CODE, prod_exposure_resource_id)\n",
    "    test_elt = irp_client.analysis.get_elt(test_analysis_id, PERSPECTIVE_CODE, test_exposure_resource_id)\n",
    "    elt_result = compare_datasets(prod_elt, test_elt, 'eventId', 'ELT', ELT_FIELDS, RELATIVE_TOLERANCE)\n",
    "    results.append(elt_result)\n",
    "    print(f\"  Production: {len(prod_elt)} events\")\n",
    "    print(f\"  Test: {len(test_elt)} events\")\n",
    "except Exception as e:\n",
    "    results.append(ComparisonResult(\n",
    "        endpoint='ELT', passed=False, total_records_prod=0, total_records_test=0,\n",
    "        differences=[], missing_in_test=[], extra_in_test=[], error=str(e)\n",
    "    ))\n",
    "    print(f\"  Error: {e}\")\n",
    "\n",
    "# --- PLT (optional, HD only) ---\n",
    "if INCLUDE_PLT:\n",
    "    print(\"\\nFetching PLT...\")\n",
    "    try:\n",
    "        prod_plt = irp_client.analysis.get_plt(prod_analysis_id, PERSPECTIVE_CODE, prod_exposure_resource_id)\n",
    "        test_plt = irp_client.analysis.get_plt(test_analysis_id, PERSPECTIVE_CODE, test_exposure_resource_id)\n",
    "        plt_result = compare_datasets(prod_plt, test_plt, 'eventId', 'PLT', PLT_FIELDS, RELATIVE_TOLERANCE)\n",
    "        results.append(plt_result)\n",
    "        print(f\"  Production: {len(prod_plt)} events\")\n",
    "        print(f\"  Test: {len(test_plt)} events\")\n",
    "    except Exception as e:\n",
    "        results.append(ComparisonResult(\n",
    "            endpoint='PLT', passed=False, total_records_prod=0, total_records_test=0,\n",
    "            differences=[], missing_in_test=[], extra_in_test=[], error=str(e)\n",
    "        ))\n",
    "        print(f\"  Error: {e}\")\n",
    "\n",
    "print(\"\\nComparison complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-header",
   "metadata": {},
   "source": [
    "## 6. Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "results-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANALYSIS VALIDATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Production Analysis ID: 35839\n",
      "Test Analysis ID:       35662\n",
      "Perspective:            GU\n",
      "\n",
      "------------------------------------------------------------\n",
      "Endpoint Results:\n",
      "------------------------------------------------------------\n",
      "  [X] Statistics: FAIL (1 value differences)\n",
      "  [OK] EP Metrics: PASS\n",
      "  [OK] ELT: PASS\n",
      "\n",
      "============================================================\n",
      "OVERALL: FAIL\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ANALYSIS VALIDATION RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"Production Analysis ID: {PRODUCTION_APP_ANALYSIS_ID}\")\n",
    "print(f\"Test Analysis ID:       {TEST_APP_ANALYSIS_ID}\")\n",
    "print(f\"Perspective:            {PERSPECTIVE_CODE}\")\n",
    "print()\n",
    "print(\"-\" * 60)\n",
    "print(\"Endpoint Results:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "overall_pass = True\n",
    "for result in results:\n",
    "    status = \"PASS\" if result.passed else \"FAIL\"\n",
    "    icon = \"[OK]\" if result.passed else \"[X]\" \n",
    "    \n",
    "    details = \"\"\n",
    "    if result.error:\n",
    "        details = f\" (Error: {result.error})\"\n",
    "    elif not result.passed:\n",
    "        issues = []\n",
    "        if result.differences:\n",
    "            issues.append(f\"{len(result.differences)} value differences\")\n",
    "        if result.missing_in_test:\n",
    "            issues.append(f\"{len(result.missing_in_test)} missing in test\")\n",
    "        if result.extra_in_test:\n",
    "            issues.append(f\"{len(result.extra_in_test)} extra in test\")\n",
    "        details = f\" ({', '.join(issues)})\"\n",
    "    \n",
    "    print(f\"  {icon} {result.endpoint}: {status}{details}\")\n",
    "    \n",
    "    if not result.passed:\n",
    "        overall_pass = False\n",
    "\n",
    "print()\n",
    "print(\"=\" * 60)\n",
    "overall_status = \"PASS\" if overall_pass else \"FAIL\"\n",
    "print(f\"OVERALL: {overall_status}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "details-header",
   "metadata": {},
   "source": [
    "## 7. Detailed Differences (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "details",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Statistics DIFFERENCES\n",
      "============================================================\n",
      "\n",
      "Value differences (1 records with differences):\n",
      "\n",
      "  Key: record_0\n",
      "    cv:\n",
      "      prod: 2.794750131263206\n",
      "      test: 2.9412708724306715\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    if result.passed:\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{result.endpoint} DIFFERENCES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    if result.error:\n",
    "        print(f\"\\nError: {result.error}\")\n",
    "        continue\n",
    "    \n",
    "    # Missing records\n",
    "    if result.missing_in_test:\n",
    "        print(f\"\\nRecords in PRODUCTION but missing in TEST ({len(result.missing_in_test)} total):\")\n",
    "        shown = result.missing_in_test[:MAX_DIFFERENCES_TO_SHOW]\n",
    "        for key in shown:\n",
    "            print(f\"  - {key}\")\n",
    "        if len(result.missing_in_test) > MAX_DIFFERENCES_TO_SHOW:\n",
    "            print(f\"  ... and {len(result.missing_in_test) - MAX_DIFFERENCES_TO_SHOW} more\")\n",
    "    \n",
    "    # Extra records\n",
    "    if result.extra_in_test:\n",
    "        print(f\"\\nRecords in TEST but not in PRODUCTION ({len(result.extra_in_test)} total):\")\n",
    "        shown = result.extra_in_test[:MAX_DIFFERENCES_TO_SHOW]\n",
    "        for key in shown:\n",
    "            print(f\"  - {key}\")\n",
    "        if len(result.extra_in_test) > MAX_DIFFERENCES_TO_SHOW:\n",
    "            print(f\"  ... and {len(result.extra_in_test) - MAX_DIFFERENCES_TO_SHOW} more\")\n",
    "    \n",
    "    # Value differences\n",
    "    if result.differences:\n",
    "        print(f\"\\nValue differences ({len(result.differences)} records with differences):\")\n",
    "        shown = result.differences[:MAX_DIFFERENCES_TO_SHOW]\n",
    "        for diff in shown:\n",
    "            print(f\"\\n  Key: {diff['key']}\")\n",
    "            for field_diff in diff['differences']:\n",
    "                print(f\"    {field_diff['field']}:\")\n",
    "                print(f\"      prod: {field_diff['prod_value']}\")\n",
    "                print(f\"      test: {field_diff['test_value']}\")\n",
    "        if len(result.differences) > MAX_DIFFERENCES_TO_SHOW:\n",
    "            print(f\"\\n  ... and {len(result.differences) - MAX_DIFFERENCES_TO_SHOW} more records with differences\")\n",
    "\n",
    "if overall_pass:\n",
    "    print(\"\\nNo differences found - all endpoints match!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
